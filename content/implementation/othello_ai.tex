\begin{lstlisting}[language=Python]
%%HTML
<style>
.container { width:100% }
</style>
\end{lstlisting}

\hypertarget{implementierung-einer-kuxfcnstlichen-intelligenz-fuxfcr-das-spiel-othello}{%
\section{Implementierung einer Künstlichen Intelligenz für das Spiel
Othello}\label{implementierung-einer-kuxfcnstlichen-intelligenz-fuxfcr-das-spiel-othello}}

\hypertarget{initiale-konfiguration}{%
\subsection{Initiale Konfiguration}\label{initiale-konfiguration}}

Importieren von Abhängigkeiten und Konfiguration

Die Implementierung der Spielelogik findet in einem separaten Jupyter
Notebook statt. Diese wird deshalb hier zunächst ausgeführt.

\begin{lstlisting}[language=Python]
import math
import copy
import numpy as np
import random
\end{lstlisting}

\begin{lstlisting}[language=Python]
utilities = {WHITE: '-', BLACK: '-'}
\end{lstlisting}

\hypertarget{heuristiken}{%
\subsection{Heuristiken}\label{heuristiken}}

Zum Abschätzen der Nützlichkeit eines Spielzustands wird eine Heuristik
benötigt. Im folgenden sind einige solcher Heuristiken implementiert. Da
Weiß der maximierende Spieler, und Schwarz der minimierende Spieler ist
repräsentiert ein höherer Wert der Heuristik einen für Weiß
vorteilhaften Zug, während ein niedriger Wert einen Vorteil für Schwarz
repräsentiert. Die Werte der Heuristik liegen zwischen -1 und 1, wobei
die Randwerte einen garantierten Sieg für den jeweiligen Spieler
darstellen. Der Wert 0 steht für einen für beide Spieler gleich guten
Spielzustand.

Da das Ziel des Spiels Othello ist, zum Ende des Spiels mehr Steine als
der Gegner auf dem Spielfeld zu haben, ist es naheliegend, die Differez
der Anzahlen von Steinen beider Spieler zur Abschätzung eines Zuges zu
verwenden. Genau das macht die Disk-Count-Heuristik indem sie die
Differenz der Steine, beider Spieler berechnet und den resultierenden
Wert zur Normalisierung durch die maximale Anzahl an Steinen teilt. Bei
genauerer Betrachtung ist es jedoch, gerade zu Beginn des Spiels nicht
immer vorteilhaft, den Vorsprung an Steinen zu maximieren.

\begin{lstlisting}[language=Python]
def disc_count_heuristic(state):
    return (count_disks(state, WHITE) - count_disks(state, BLACK)) / 64
\end{lstlisting}

Eine weitere Heuristik ist die Mobilität der Spieler. Diese gibt an wie
viele mögliche Züge ein Spieler gegenüber dessen Gegner machen kann. Die
Idee bei dieser Heuristik ist, dass ein Spieler dadurch seine Freiheit
maximiert, während die Freiheit des Gegners durch eine geringe Anzahl an
Zügen eingeschränkt wird. Die Mobilitäts-Heuristik gibt an wie viele
möglicher Züge mehr Weiß gegenüber Schwarz im Aktuellen Spielzustand
hat. Auch dieser Wert wird durch Division durch die Anzahl an Feldern
normalisiert, um die Grenzen von -1 und 1 einzuhalten.

\begin{lstlisting}[language=Python]
def mobility_heuristic(state):
    if state.turn == WHITE:
        return (len(state.possible_moves) - len(get_possible_moves(state, BLACK))) / 64
    else:
        return (len(get_possible_moves(state, WHITE)) - len(state.possible_moves)) / 64
\end{lstlisting}

Beim Spielen von Othello fällt auf, dass es bestimmte Felder gibt, deren
Belegung von Vorteil ist, sowie einige, deren Belegung eher nachteilhaft
ist. Diese Eigenschaft macht sich die Cowthello-Heuristik zu Nutze.
Diese weist jedem Feld einen Wert zu der angibt, wie Vorteilhaft der
Besitz dieses Feldes ist, bzw. wie Nachteilhaft die Belegung des Feldes
durch den Gegner ist. Diese Gewichte werden dann mit der aktuellen
Belegung des Spielfelds multipliziert und die Ergebnisse anschließend
aufsummiert. Der resultierende Wert schätzt dann den Nutzen der
aktuellen Position ein. Auch bei dieser Heuristik findet eine
Normalisierung statt.

Aufgrund der Symmetrie des Othello Spielfeldes ist es für die
Weight-Heuristik nicht nötig, für jedes Feld einzeln dessen Gewicht
anzugeben. Stattdessen werden ausschließlich die Gewichte für ein
Viertel des Spielfelds angegeben und dieses anschließend gespiegelt. Die
Funktion \passthrough{\lstinline!gen\_cowthello\_matrix!} generiert dann
die Gewichte-Matrix für das gesamte Feld und führt auch die
Normalisierung durch. Dabei werden die Gewichte aus dem Online-Othello
Programm Cowthello verwendet. Cowthello ist unter der URL
\url{https://www.aurochs.org/games/cowthello/} verfügbar.

\begin{lstlisting}[language=Python]
def gen_cowthello_matrix():
    quarter = np.array([
        [100, -25, 25, 10],
        [-25, -50,  1,  1],
        [ 25,   1, 50,  5],
        [ 10,   1,  5,  1]
    ])
    top_half = np.hstack((quarter, np.flip(quarter, axis=1)))
    bottom_half = np.flip(top_half, axis=0)
    raw_matrix = np.vstack((top_half, bottom_half))
    max_possible = np.sum(np.absolute(raw_matrix))
    return np.true_divide(raw_matrix, max_possible)

cowthello_weights = gen_cowthello_matrix()
\end{lstlisting}

\begin{lstlisting}[language=Python]
def cowthello_heuristic(state):
    return np.sum(np.multiply(state.board, cowthello_weights))
\end{lstlisting}

Die oben implementierten Heuristiken bewerten jeweils nur ein Merkmal
der aktuellen Spielsitation. Durch eine Kombination mehrerer dieser
Heuristiken können mehrere Merkmale gleichzeitig betrachtet werden. Die
Kombination der Heuristiken kann in Abhängigkeit der aktuellen
Spielphase geschehen. Beispielsweise wird zu Ende des Spiels die
Disc-Count-Heuristik wichtiger.

\begin{lstlisting}[language=Python]
def combined_heuristic(state):
    if state.num_pieces >= 50:
        return disc_count_heuristic(state)
    mobility = mobility_heuristic(state)
    cowthello = cowthello_heuristic(state)
    return 0.5 * mobility + 0.5 * cowthello
\end{lstlisting}

\hypertarget{implementierung-der-strategien}{%
\subsection{Implementierung der
Strategien}\label{implementierung-der-strategien}}

\hypertarget{zufuxe4llige-ki}{%
\subsubsection{Zufällige KI}\label{zufuxe4llige-ki}}

Die zufällige KI bewertet den Nutzen aller Züge gleich, gibt also immer
den Wert \passthrough{\lstinline!0!} zurück.

\begin{lstlisting}[language=Python]
def random_ai(state, depth, heuristic, alpha, beta):
    return 0
\end{lstlisting}

\hypertarget{minimax-ki}{%
\subsubsection{Minimax KI}\label{minimax-ki}}

Diese KI verwendet den Minimax-Algorithmus zur Bestimmung der
Nützlichkeit eines Zuges.

\begin{lstlisting}[language=Python]
debug_mm_count = 0


def minimax(state, depth, heuristic, alpha, beta):
    global debug_mm_count
    if state.game_over:
        return get_winner(state)
    if depth == 0:
        debug_mm_count += 1
        return heuristic(state)

    if state.turn == WHITE:
        # maximizing
        utility = -math.inf
    else:
        # minimizing
        utility = math.inf

    for move in get_possible_moves(state, state.turn):
        tmp_state = copy.deepcopy(state)
        make_move(tmp_state, move)
        tmp_utility = minimax(tmp_state, depth - 1, heuristic, None, None)
        if state.turn == WHITE:
            # maximizing
            utility = max(utility, tmp_utility)
        else:
            # minimizing
            utility = min(utility, tmp_utility)
    return utility
\end{lstlisting}

\hypertarget{alpha-beta-ki}{%
\subsubsection{Alpha-Beta KI}\label{alpha-beta-ki}}

Diese KI verwended den Minimax Algorithmus mit der Optimierung
Alpha-Beta Pruning, um die Nützlichkeit eines Spielzustands zu
bestimmen.

Zum Merken vorheriger Ausführungen von wird das Dictionary
\passthrough{\lstinline!transposition\_table!} verwendet. Dies ist
gerade bei der Verwendung von Iterative Deepening für das Move-Ordering
vorteilhaft. Der Schlüssel des Dictionaries besteht aus dem Zustand des
Spielbretts, dem Spieler, der an der Reihe ist und der verwendeten
Heuristik.

\begin{lstlisting}[language=Python]
transposition_table = {}
\end{lstlisting}

Die Funktion \passthrough{\lstinline!alphabeta!} implementiert den
Minimax Algorithmus mit Alpha-Beta-Pruning

\begin{lstlisting}[language=Python]
debug_ab_count = 0


def alphabeta(state, depth, heuristic, alpha, beta):
    global debug_ab_count
    if state.game_over:
        return get_winner(state)
    if depth == 0:
        debug_ab_count += 1
        return heuristic(state)

    moves = get_possible_moves(state, state.turn)
    child_states = [make_move(copy.deepcopy(state), move) for move in moves]
    ordered_moves = []
    for child_state in child_states:
        cached = transposition_table.get((child_state.board.tobytes(), child_state.turn, heuristic), None)
        if cached != None:
            ordered_moves.append((cached, child_state))
        else:
            ordered_moves.append((heuristic(state), child_state))
    ordered_moves.sort(reverse=(state.turn == WHITE))

    if state.turn == WHITE:
        # maximizing
        utility = -math.inf
    else:
        # minimizing
        utility = math.inf

    for (_, tmp_state) in ordered_moves:
        tmp_utility = alphabeta(tmp_state, depth - 1, heuristic, alpha, beta)
        transposition_table[(tmp_state.board.tobytes(),
                             tmp_state.turn, heuristic)] = tmp_utility

        if state.turn == WHITE:
            # maximizing
            utility = max(utility, tmp_utility)
            alpha = max(alpha, utility)
        else:
            # minimizing
            utility = min(utility, tmp_utility)
            beta = min(beta, utility)
        if alpha >= beta:
            break  # alphabeta pruning
    return utility
\end{lstlisting}

\hypertarget{probcut-ki}{%
\subsubsection{ProbCut KI}\label{probcut-ki}}

An dieser Stelle beginnt die Implementierung der Künstlichen Intelligenz
mittels des Minimax Algorithmus und ProbCut

\begin{lstlisting}[language=Python]
PERCENTILE = 1.5  # 93.3%
PROBCUT_DEEP_DEPTH = 4
PROBCUT_SHALLOW_DEPTH = 2
\end{lstlisting}

\begin{lstlisting}[language=Python]
debug_pc_count = 0


def probcut(state, depth, heuristic, alpha, beta):
    global debug_pc_count
    if state.game_over:
        return get_winner(state)
    if depth == 0:
        debug_pc_count += 1
        return heuristic(state)

    if depth == PROBCUT_DEEP_DEPTH - 1:
        if state.num_pieces <= 45:  # if there are not more than 45 pieces on the board sigma can be calculated based on statistic values with the combined_heuristic
            sigma = state.num_pieces * 0.001948723205790693 - 0.001604049443613545
            # v >= beta with prob. of at least p? yes => cutoff
            bound = PERCENTILE * sigma + beta
            if probcut(state, PROBCUT_SHALLOW_DEPTH, heuristic, bound-1, bound) >= bound:
                return beta

            # v <= alpha with prob. of at least p? yes => cutoff
            bound = -PERCENTILE * sigma + alpha
            if probcut(state, PROBCUT_SHALLOW_DEPTH, heuristic, bound, bound+1) <= bound:
                return alpha

    moves = get_possible_moves(state, state.turn)
    child_states = [make_move(copy.deepcopy(state), move) for move in moves]
    ordered_moves = []
    for child_state in child_states:
        cached = transposition_table.get((child_state.board.tobytes(), child_state.turn, heuristic), None)
        if cached != None:
            ordered_moves.append((cached, child_state))
        else:
            ordered_moves.append((heuristic(state), child_state))
    ordered_moves.sort(reverse=(state.turn == WHITE))

    if state.turn == WHITE:
        # maximizing
        utility = -math.inf
    else:
        # minimizing
        utility = math.inf

    for (_, tmp_state) in ordered_moves:
        tmp_utility = probcut(tmp_state, depth - 1, heuristic, alpha, beta)
        transposition_table[(tmp_state.board.tobytes(),
                             tmp_state.turn, heuristic)] = tmp_utility

        if state.turn == WHITE:
            # maximizing
            utility = max(utility, tmp_utility)
            alpha = max(alpha, utility)
        else:
            # minimizing
            utility = min(utility, tmp_utility)
            beta = min(beta, utility)
        if alpha >= beta:
            break  # alphabeta pruning
    return utility
\end{lstlisting}

\hypertarget{durchfuxfchren-der-zuxfcge}{%
\subsubsection{Durchführen der Züge}\label{durchfuxfchren-der-zuxfcge}}

Die folgenden Funktionen berechnen mit einer angegebenen KI den nächsten
Zug und wenden diesen auf den Zustand an. Der Wert
\passthrough{\lstinline!SELECTION\_TOLERANCE!} gibt an, wie viel der
Nutzen eines Zuges vom besten Nutzen abweichen darf, um dennoch
ausgewählt werden zu können.

\begin{lstlisting}[language=Python]
SELECTION_TOLERANCE = 0.0025
\end{lstlisting}

Die Funktion \passthrough{\lstinline!ai\_make\_move!} führt auf dem
Zustand \passthrough{\lstinline!state!} den besten, durch den
Algorithmus \passthrough{\lstinline!ai!} bestimmten, Zug aus. Für alle
möglichen Züge wird die Nützlichkeit des resultierenden Zustands
mithilfe des Algorithmus \passthrough{\lstinline!ai!} bestimmt. Aus
allen Zügen wird einer der Züge ausgewählt, der für den Spieler den
optimaleln Nutzen hat.

\begin{lstlisting}[language=Python]
def ai_make_move(ai, state, depth, heuristic):
    global utilities
    if state.game_over:
        return
    scored_moves = []
    for move in state.possible_moves:
        utility = ai(make_move(copy.deepcopy(state), move), depth-1, heuristic, -math.inf, math.inf)
        scored_moves.append((utility, move))
    if state.turn == WHITE:
        # maximizing
        best_score, _ = max(scored_moves)
    else:
        # minimizing
        best_score, _ = min(scored_moves)
    utilities[state.turn] = best_score
    top_moves = [move for move in scored_moves if abs(move[0] - best_score) <= SELECTION_TOLERANCE]
    best_move = random.choice(top_moves)[1]
    make_move(state, best_move)
\end{lstlisting}

Beim Iterative Deepening wird der Alphabeta-Algorithmus nacheinander mit
steigender Tiefe ausgeführt. Die Werte aus der vorherigen Ausführung
werden dann für das Move-Ordering verwendet. Dadurch das bessere
Move-Ordering können beim Alpha-Beta-Pruning mehr Zweige abgeschnitten
werden. Dadurch kann trotz der mehrfachen Ausführung eine höhere
Performanz bei gleichem Ergebnis erreicht werden. Ein weiterer Vorteil
ist, dass das Iterative Deepening jederzeit abgebrochen werden kann.
Dadurch kann in einer vorgegebenen Zeit, die in dieser Zeit maximal
mögliche Suchtiefe erreicht werden.

\begin{lstlisting}[language=Python]
def ai_make_move_id(ai, state, depth, heuristic):
    global utilities
    if state.game_over:
        return
    best_move = None
    current_depth = 1
    while current_depth <= depth:
        scored_moves = []
        for move in state.possible_moves:
            utility = ai(make_move(copy.deepcopy(state), move), current_depth-1, heuristic, -math.inf, math.inf)
            scored_moves.append((utility, move))
        if state.turn == WHITE:
            # maximizing
            best_score, _ = max(scored_moves)
        else:
            # minimizing
            best_score, _ = min(scored_moves)
        utilities[state.turn] = best_score
        top_moves = [move for move in scored_moves if abs(move[0] - best_score) <= SELECTION_TOLERANCE]
        best_move = random.choice(top_moves)[1]
        current_depth += 1
    make_move(state, best_move)
\end{lstlisting}

Beim Iterative Deepening ist jederzeit der beste Zug der letzten
Suchtiefe bekannt. Daher kann der Algorithmus verwendet werden, um in
einer vorgegebenen Zeit mit größtmöglicher Suchtiefe zu suchen. Dazu
wird die Dauer des nächsten Zugs anhand der Dauer des letzten Zugs
geschätzt. Somit kann es vorkommen, dass die Berechnung etwas länger
oder kürzer als die gegebene Zeit dauert. Dafür wird das Ergebnis aller
Berechnungen verwendet und die letzte Suche muss nicht abgebrochen
werden.

\begin{lstlisting}[language=Python]
SECS_PER_MOVE = 30


def ai_make_move_id_timelimited(ai, state, depth, heuristic):
    global utilities
    if state.game_over:
        return
    best_move = None
    depth = 1
    last_time = 1
    second_last_time = 1
    factor = 0
    start = time.time()
    while depth <= 64 - state.num_pieces and SECS_PER_MOVE - (time.time() - start) >= factor * last_time:
        last_time_start = time.time()
        scored_moves = []
        for move in state.possible_moves:
            utility = ai(make_move(copy.deepcopy(state), move), depth-1, heuristic, -math.inf, math.inf)
            scored_moves.append((utility, move))
        if state.turn == WHITE:
            # maximizing
            best_score, _ = max(scored_moves)
        else:
            # minimizing
            best_score, _ = min(scored_moves)
        utilities[state.turn] = best_score
        top_moves = [move for move in scored_moves if abs(move[0] - best_score) <= SELECTION_TOLERANCE]
        best_move = random.choice(top_moves)[1]
        second_last_time = last_time
        last_time = time.time() - last_time_start
        factor = min(last_time / second_last_time, 3)
        depth += 1
    print("Reached depth", depth-1, "in", time.time() - start, "seconds")
    make_move(state, best_move)
\end{lstlisting}
