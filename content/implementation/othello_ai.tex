\hypertarget{ki-implementierung-othello_ai.ipynb}{%
\section{KI Implementierung
(othello\_ai.ipynb)}\label{ki-implementierung-othello_ai.ipynb}}

\label{sec:aiimpl}

\begin{lstlisting}[language=Python]
%%HTML
<style>
.container { width:100% }
</style>
\end{lstlisting}

Dieses Kapitel beschreibt die Implemenentierung der Künstlichen
Intelligenz. Es werden mehrere, voneinander verschiedene, Strategien
implementiert, um einen Vergleich von diesen zu ermöglichen.

\hypertarget{importieren-der-externen-abhuxe4ngigkeiten}{%
\subsection{Importieren der externen
Abhängigkeiten}\label{importieren-der-externen-abhuxe4ngigkeiten}}

Zur Initialisierung der Parameter \passthrough{\lstinline!alpha!} und
\passthrough{\lstinline!beta!} in der mit Alpha-Beta Pruning optimierten
Minimax Strategie werden die Konstanten
\passthrough{\lstinline!math.inf!} und
\passthrough{\lstinline!-math.inf!} benötigt. Sie stehen jeweils für den
maximalen und den minimalen Wert, den eine Fließkommazahl annehmen kann.
Die Konstanten werden von der Python Standardbibliothek in dem Modul
\passthrough{\lstinline!math!} bereitgestellt

Das Modul \passthrough{\lstinline!random!} wird im Rahmen dieser
Implementierung für mehrere Zwecke genutzt. Zum Einen zur
Implementierung der \passthrough{\lstinline!random\_ai!}, einer
Strategie, welche immer einen zufälligen Zug wählt. Und außerdem, um die
auf Minimax basierenden Strategien nichtdeterministisch zu machen.

\begin{lstlisting}[language=Python]
import math
import random
\end{lstlisting}

Die Implementierung der Künstlichen Intelligenz baut auf der
Implementierung der Spielelogik von Othello auf. Das entsprechende
Notebook muss also hier ausgeführt werden.

\begin{lstlisting}[language=Python]
%run othello_game.ipynb
\end{lstlisting}

Die aktuellen Nutzen-Werte der beiden Spieler werden in der globalen
Variable utilities gespeichert, sodass diese in der GUI angezeigt werden
können. Diese Werte werden in den entsprechenden Funktionen der
Strategien aktualisiert.

\begin{lstlisting}[language=Python]
utilities = {WHITE: '-', BLACK: '-'}
\end{lstlisting}

\hypertarget{heuristiken}{%
\subsection{Heuristiken}\label{heuristiken}}

Zum Abschätzen der Nützlichkeit eines Spielzustands wird eine Heuristik
benötigt. Im folgenden sind einige solcher Heuristiken implementiert. Da
Weiß der maximierende Spieler, und Schwarz der minimierende Spieler ist,
repräsentiert ein höherer Wert der Heuristik einen für Weiß
vorteilhaften Zug, während ein niedriger Wert einen Vorteil für Schwarz
repräsentiert. Die Werte aller Heuristiken liegen zwischen \(-1\) und
\(1\), wobei diese Randwerte einen garantierten Sieg für den jeweiligen
Spieler darstellen. Der Wert \(0\) steht für einen für beide Spieler
gleich guten Spielzustand.

Da das Ziel des Spiels Othello ist, zum Ende des Spiels mehr Steine als
der Gegner auf dem Spielfeld zu haben, ist es naheliegend, die Differenz
der Anzahlen von Steinen beider Spieler zur Abschätzung eines Zuges zu
verwenden. Genau das macht die Disk-Count-Heuristik indem sie die
Differenz der Anzahlen von Steinen, beider Spieler berechnet und den
resultierenden Wert zur Normalisierung durch die maximale Anzahl an
Steinen teilt. Bei genauerer Betrachtung ist es jedoch, gerade zu Beginn
des Spiels, nicht immer vorteilhaft, den Vorsprung an Steinen zu
maximieren.

\begin{lstlisting}[language=Python]
def disc_count_heuristic(state):
    return (count_disks(state, WHITE) - count_disks(state, BLACK)) / 64
\end{lstlisting}

Eine weitere Heuristik ist die Mobilität der Spieler. Diese gibt an, wie
viele mögliche Züge ein Spieler im Vergleich zum Gegner hat. Die Idee
hinter dieser Heuristik ist, dass ein Spieler dadurch seine Freiheit
maximiert, während die Freiheit des Gegners durch eine geringe Anzahl an
Zügen eingeschränkt wird. Die Mobilitäts-Heuristik gibt an wie viele
mögliche Züge mehr Weiß gegenüber Schwarz im Aktuellen Spielzustand hat.
Auch dieser Wert wird durch Division durch die Anzahl an Feldern
normalisiert, um die Grenzen von \(-1\) und \(1\) einzuhalten. Zu
beachten ist hier, dass auch die Anzahl möglicher Züge für einen Spieler
bestimmt wird, der im Spielzustand gar nicht am Zug ist. Dies ist wirkt
zunächst sematisch nicht sinvoll, hat sich jedoch, wie in
\autoref{sec:currentmobility} gezeigt, im Vergleich gegenüber möglichen
Alternativen, wie der Verwendung einer durchschnittlichen Mobilität, als
effektiv bewiesen.

\begin{lstlisting}[language=Python]
def mobility_heuristic(state):
    if state.turn == WHITE:
        return (len(state.possible_moves) -
                len(get_possible_moves(state, BLACK))) / 64
    else:
        return (len(get_possible_moves(state, WHITE)) -
                len(state.possible_moves)) / 64
\end{lstlisting}

Nicht nur die aktuelle, sondern auch die potentielle Mobilität kann vor
allem in frühen Phasen des Spiels wichtig für die Bewertung einer
Position sein. Die Funktion
\passthrough{\lstinline!pot\_mob\_heuristic!} berechnet für einen
Zustand \passthrough{\lstinline!state!} die Differenz der potentiellen
Mobilität beider Spieler. Die potentielle Mobilität eines Spielers ist
gegeben durch die Summe aller freien Felder um gegnerische Spielsteine,
da Michael Buro dieses Merkmal als in seiner Dissertation als beste
Metrik für die Potentielle Mobilität ausgemacht hat
\cite[S. 9]{evaluationfunctions}. Das Ergebnis wird durch \(3.5\)
geteilt, da es im Durchschnitt \(3.5\) mal so viele potentielle Züge wie
tatsächliche Züge gibt.

\begin{lstlisting}[language=Python]
def pot_mob_heuristic(state):
    board = list(state.board)
    fields = 0
    for (x,y) in state.frontier:
        for dx, dy in directions:
            xi = x + dx
            yi = y + dy
            if 0 <= xi < 8 and 0 <= yi < 8 and board[xi][yi] != 0:
                fields -= board[xi][yi]
    # Im Durchschnitt gibt es 3.5 Mal mehr potentielle wie tatsächliche Züge
    fields /= 3.5 
    return fields / 64
\end{lstlisting}

Die Funktion \passthrough{\lstinline!combined\_mobility\_heuristic!}
kombiniert die aktuelle und potentielle Mobilität, wobei zu Beginn des
Spiels die potentielle Mobilität stärker gewichtet wird und gegen Ende
des Spiels die aktuelle Mobilität. Michael Buro beschreibt in seiner
Dissertation, dass die potentielle Mobilität bis 36 Spielsteine auf dem
Feld liegen wichtiger für die Bewertung ist, als die aktuelle Mobilität
\cite[S. 9]{evaluationfunctions}. Eigene Tests, welche in
\autoref{sec:combinedmobility} beschrieben sind, ergeben, dass eine
lineare Kombination der beiden Merkmale zu einem guten Ergebnis führt.

\begin{lstlisting}[language=Python]
def combined_mobility_heuristic(state):
    act = mobility_heuristic(state)
    pot = pot_mob_heuristic(state)
    return (1 - state.num_pieces / 50) * pot + (state.num_pieces / 50) *  act
\end{lstlisting}

Beim Spielen von Othello fällt auf, dass es bestimmte Felder gibt, deren
Belegung von Vorteil ist, sowie einige, deren Belegung eher nachteilhaft
ist. Diese Eigenschaft macht sich die Cowthello-Heuristik zu Nutze.
Diese weist jedem Feld einen Wert zu der angibt, wie Vorteilhaft der
Besitz dieses Feldes ist, bzw. wie Nachteilhaft die Belegung des Feldes
durch den Gegner ist. Diese Gewichte werden mit der aktuellen Belegung
des Spielfelds multipliziert und die Ergebnisse anschließend
aufsummiert. Der resultierende Wert schätzt dann den Nutzen der
aktuellen Position ein. Auch bei dieser Heuristik findet eine
Normalisierung statt.

Aufgrund der Symmetrie des Othello Spielfeldes ist es für die
Weight-Heuristik nicht nötig, für jedes Feld einzeln dessen Gewicht
anzugeben. Stattdessen werden ausschließlich die Gewichte für ein
Viertel des Spielfelds angegeben und dieses anschließend gespiegelt. Die
Funktion \passthrough{\lstinline!gen\_cowthello\_matrix!} generiert dann
die Gewichte-Matrix für das gesamte Feld und führt auch die
Normalisierung durch. Dabei werden die Gewichte aus dem Online-Othello
Programm Cowthello verwendet. Cowthello ist unter der URL
\url{https://www.aurochs.org/games/cowthello/} verfügbar.

\begin{lstlisting}[language=Python]
def gen_cowthello_matrix():
    quarter = np.array([
        [100, -25, 25, 10],
        [-25, -50,  1,  1],
        [ 25,   1, 50,  5],
        [ 10,   1,  5,  1]
    ])
    top_half = np.hstack((quarter, np.flip(quarter, axis=1)))
    bottom_half = np.flip(top_half, axis=0)
    raw_matrix = np.vstack((top_half, bottom_half))
    max_possible = np.sum(np.absolute(raw_matrix))
    return np.true_divide(raw_matrix, max_possible)

cowthello_weights = gen_cowthello_matrix()
\end{lstlisting}

Die Funktion \passthrough{\lstinline!cowthello\_heuristic!} bestimmt aus
einem Spielzustand und der aufgestellten Gewichtematrix
\passthrough{\lstinline!cowthello\_weights!} die gewichtete Summe,
welche als Heuristik genutzt wird.

\begin{lstlisting}[language=Python]
def cowthello_heuristic(state):
    return np.sum(np.multiply(state.board, cowthello_weights))
\end{lstlisting}

Die Cowthello Heuristik wertet den Besitz einiger Felder, wie zum
Beispiel die an die Ecken des Spielfeldes angrenzenden Felder, als
negativ, da der Gegner dadurch wertvolle Felder, wie die Ecken des
Spielfelds, erlangen kann. Wenn die Ecke jedoch bereits besetzt ist,
spielt das keine Rolle mehr. Die
\passthrough{\lstinline!cowthello\_safe\_heuristic!} versucht dies zu
berücksichtigen. Zunächst wird dabei in der Funktion
\passthrough{\lstinline!safe\_in\_corner!} ausgehend von einer durch
\passthrough{\lstinline!rdir!} und \passthrough{\lstinline!cdir!}
angegeben Ecke des Spielfeldes \passthrough{\lstinline!board!} bestimmt,
welche Steine des Spielers \passthrough{\lstinline!player!} nicht mehr
umgedreht werden können. Alle dadurch als sicher bestimmten Felder,
werden in der Boolean-Matrix safe auf den Wert
\passthrough{\lstinline!True!} gesetzt.

\begin{lstlisting}[language=Python]
def safe_in_corner(board, safe, player, rdir, cdir):
    safe_in_row = 9
    rows = range(8) if rdir == 1 else reversed(range(8))
    for row in rows:
        i = np.argmax(board[row,::cdir] != player)
        safe_in_row = min(i, safe_in_row - 1)
        if safe_in_row == 0:
            break
        safe[row,::cdir][:safe_in_row] = True
    safe_in_col = 9
    cols = range(8) if cdir == 1 else reversed(range(8))
    for col in cols:
        i = np.argmax(board[::rdir,col] != player)
        safe_in_col = min(i, safe_in_col - 1)
        if safe_in_col == 0:
            break
        safe[::rdir,col][:safe_in_col] = True
\end{lstlisting}

In der Funktion \passthrough{\lstinline!safe\_pieces!} wird die Funktion
\passthrough{\lstinline!safe\_in\_corner!} für alle Ecken des Spielfelds
\passthrough{\lstinline!board!} und dem Spieler
\passthrough{\lstinline!player!} aufgerufen. Die sicheren Felder werden
in der Boolean Matrix \passthrough{\lstinline!safe!} gesammelt, welche
von der Funktion zurückgegeben wird.

\begin{lstlisting}[language=Python]
def safe_pieces(state, player):
    board = state.board
    safe = np.zeros((8, 8), dtype=np.bool)
    safe_in_corner(board, safe, player, 1, 1)
    safe_in_corner(board, safe, player, 1,-1)
    safe_in_corner(board, safe, player,-1, 1)
    safe_in_corner(board, safe, player,-1,-1)
    return safe
\end{lstlisting}

Die \passthrough{\lstinline!cowthello\_safe\_heuristic!} bestimmt
zunächst für beide Spieler die bereits gesicherten Felder. Die
\passthrough{\lstinline!cowthello\_weights!} Matrix wird dann so
angepasst, dass alle gesicherten Felder positiv gewichtet werden. Dazu
wird der absolutwert der vorherigen Gewichtung verwendet. Die Berechnung
der Heuristik, funktioniert dann, wie bei der unveränderten
\passthrough{\lstinline!cowthello\_heuristic!}.

\begin{lstlisting}[language=Python]
def cowthello_safe_heuristic(state):
    black_safe = safe_pieces(state, BLACK)
    white_safe = safe_pieces(state, WHITE)
    weights = np.copy(cowthello_weights)
    weights[black_safe] = abs(weights[black_safe])
    weights[white_safe] = abs(weights[white_safe])
    return np.sum(np.multiply(state.board, weights))
\end{lstlisting}

Die oben implementierten Heuristiken bewerten jeweils nur ein Merkmal
der aktuellen Spielsitation. Durch eine Kombination mehrerer dieser
Heuristiken können mehrere Merkmale gleichzeitig betrachtet werden. Die
Kombination der Heuristiken kann auch in Abhängigkeit von der aktuellen
Spielphase geschehen.

\begin{lstlisting}[language=Python]
def combined_heuristic(state):
    mobility = combined_mobility_heuristic(state)
    cowthello = cowthello_safe_heuristic(state)
    return 0.5 * mobility + 0.5 * cowthello
\end{lstlisting}

\hypertarget{implementierung-der-strategien}{%
\subsection{Implementierung der
Strategien}\label{implementierung-der-strategien}}

Im Folgenden werden die verschiedenen Strategien der Künstlichen
Intelligenz implementiert. Diese verwenden zum Teil die, im vorherigen
Kaptitel implementierten, Heuristiken. Die Strategien bestehen jeweils
aus einer bewertenden Funktion, die den Nutzen einer Spielsituation
bestimmt, sowie aus einer aufrufenden Funktion, welche mithilfe der
bewertenden Funktion den bestmöglichen Zug herleitet. Diese Komponenten
können beliebig kombiniert werden. Im Folgenden werden zunächst die
bewerteten Funktionen implementiert.

\hypertarget{zufuxe4llige-ki}{%
\subsubsection{Zufällige KI}\label{zufuxe4llige-ki}}

Die zufällige KI bewertet den Nutzen aller Züge gleich, gibt also immer
den Wert \(0\) zurück. Da die Strategie-Funktionen völlig austauschbar
sein sollen, müssen alle diese Funktionen dieselben Eingabeparameter
haben. Im Fall der Funktion \passthrough{\lstinline!random\_ai!} wird
jedoch keiner der definierten Parameter benötigt. Der Zweck dieser
Künstlichen Intelligenz ist die Messung der Stärke, der anderen KIs.

\begin{lstlisting}[language=Python]
def random_ai(state, depth, heuristic, alpha, beta):
    return 0
\end{lstlisting}

\hypertarget{minimax-ki}{%
\subsubsection{Minimax KI}\label{minimax-ki}}

Die Minimax Strategie verwendet den unveränderten Minimax-Algorithmus,
wie er in \autoref{sec:minimax} beschrieben ist, zur Bestimmung der
Nützlichkeit eines Zuges. Eingabeparameter, sind hier der zu bewertende
Spielzustand \passthrough{\lstinline!state!}, die gewünschte Suchtiefe
\passthrough{\lstinline!depth!} sowie die zu verwendende Heuristik
\passthrough{\lstinline!heuristic!}. Die Parameter
\passthrough{\lstinline!alpha!} und \passthrough{\lstinline!beta!}
dienen, wie oben beschrieben, der Kompatibilität mit den folgenden
Strategiefunktionen und werden in der \passthrough{\lstinline!minimax!}
Funktion nicht verwendet. Der Rückgabeparameter gibt die ermittelte
Nützlichkeit des Spielzustands an.

\begin{lstlisting}[language=Python]
debug_mm_count = 0

def minimax(state, depth, heuristic, alpha, beta):
    global debug_mm_count
    if state.game_over:
        return get_utility(state)
    if depth == 0:
        debug_mm_count += 1
        return heuristic(state)

    if state.turn == WHITE:
        # maximizing
        utility = -math.inf
    else:
        # minimizing
        utility = math.inf

    for move in state.possible_moves:
        tmp_state = make_move(state, move)
        tmp_utility = minimax(tmp_state, depth - 1, heuristic, None, None)
        if state.turn == WHITE:
            # maximizing
            utility = max(utility, tmp_utility)
        else:
            # minimizing
            utility = min(utility, tmp_utility)
    return utility
\end{lstlisting}

\hypertarget{alpha-beta-ki}{%
\subsubsection{Alpha-Beta KI}\label{alpha-beta-ki}}

Diese KI verwended den Minimax Algorithmus mit der Optimierung
Alpha-Beta Pruning, welche in \autoref{sec:alphabeta} beschrieben ist,
um die Nützlichkeit eines Spielzustands zu bestimmen.

Zum Merken der Ergebnisse vorheriger Ausführungen wird das Dictionary
\passthrough{\lstinline!transposition\_table!} verwendet. Dies ist
gerade bei der Verwendung von Iterative Deepening für das Move-Ordering
vorteilhaft. Der Schlüssel des Dictionaries besteht aus dem Zustand des
Spielbretts, dem Spieler, der an der Reihe ist und der verwendeten
Heuristik.

\begin{lstlisting}[language=Python]
transposition_table = {}
\end{lstlisting}

Die Funktion \passthrough{\lstinline!alphabeta!} implementiert den
Minimax-Algorithmus mit Alpha-Beta-Pruning. Eingabeparameter der
Funktion sind der zu bewertende Spielzustand
\passthrough{\lstinline!state!}, die maximale Suchtiefe
\passthrough{\lstinline!depth!}, die zu verwendende Heuristik
\passthrough{\lstinline!heuristic!}, sowie die Werte
\passthrough{\lstinline!alpha!} und \passthrough{\lstinline!beta!}, die,
wie in \autoref{sec:alphabeta} beschrieben, jeweils die sicher
erreichbaren Nützlichkeiten für den maximierenden und minimierenden
Spieler angeben und für das Abschneiden von Zweigen verwendet werden.

\begin{lstlisting}[language=Python]
debug_ab_count = 0

def alphabeta(state, depth, heuristic, alpha, beta):
    global debug_ab_count
    if state.game_over:
        return get_utility(state)
    if depth == 0:
        debug_ab_count += 1
        return heuristic(state)

    moves = state.possible_moves
    child_states = [make_move(state, move) for move in moves]
    ordered_moves = []
    for child_state in child_states:
        cached = transposition_table.get(
            (child_state.board.tobytes(), child_state.turn, heuristic),
            (heuristic(child_state), 0)
        )
        ordered_moves.append((cached[0], child_state, cached[1]))
    ordered_moves.sort(reverse=(state.turn == WHITE))

    if state.turn == WHITE:
        # maximizing
        utility = -math.inf
    else:
        # minimizing
        utility = math.inf

    for (_, tmp_state, cached_depth) in ordered_moves:
        tmp_utility = alphabeta(tmp_state, depth - 1, heuristic, alpha, beta)
        if depth - 1 > cached_depth:
            transposition_table[
                (tmp_state.board.tobytes(),
                 tmp_state.turn, heuristic)
            ] = (tmp_utility, depth -1)

        if state.turn == WHITE:
            # maximizing
            utility = max(utility, tmp_utility)
            alpha = max(alpha, utility)
        else:
            # minimizing
            utility = min(utility, tmp_utility)
            beta = min(beta, utility)
        if alpha >= beta:
            break  # alpha-beta pruning
    return utility
\end{lstlisting}

\hypertarget{probcut-ki}{%
\subsubsection{ProbCut KI}\label{probcut-ki}}

An dieser Stelle beginnt die Implementierung der Künstlichen Intelligenz
mittels des Minimax Algorithmus, Alpha-Beta Pruning und ProbCut. Die im
Folgenden definierte Konstante \passthrough{\lstinline!PERCENTILE!}
entspricht hierbei der Variable \(p\) aus \autoref{sec:probcut}.
\passthrough{\lstinline!PROBCUT\_DEEP\_DEPTH!} entspricht der Variable
\(d\) und \passthrough{\lstinline!PROBCUT\_SHALLOW\_DEPTH!} entspricht
der Variable \(d'\) aus demselben Abschnitt dieser Arbeit.

\begin{lstlisting}[language=Python]
PERCENTILE = 1.5  # 93.3%
PROBCUT_DEEP_DEPTH = 4
PROBCUT_SHALLOW_DEPTH = 2
\end{lstlisting}

Die Implementierung der ProbCut Strategie gleicht in großen Teilen der
Implementierung der Alpha-Beta Strategie. Jedoch wird bei jedem Aufruf
mit der Tiefe \passthrough{\lstinline!PROBCUT\_DEEP\_DEPTH!}, zunächst
eine Suche mit der Tiefe
\passthrough{\lstinline!PROBCUT\_SHALLOW\_DEPTH!} durchgeführt. Anhand
der dabei ermittelten Nützlichkeit, wird entsprechend der in
\autoref{sec:probcut} beschriebenen Regeln entschieden, ob eine Tiefe
Suche durchgeführt werden muss, oder eine der beiden Grenzwerte
\passthrough{\lstinline!alpha!} oder \passthrough{\lstinline!beta!}
zurückgegeben werden können. Zur Abschätzung der für den Probcut
Algorithmus benötigten Standardabweichung
\passthrough{\lstinline!sigma!}, wird eine quadratische Funktion in
Abhängigkeit von der Anzahl an steinen auf dem Spielfeld verwendet.
Diese wird im folgenden \autoref{sec:pcsigma} hergeleitet. Die Eingabe-
und der Rückgabeparameter gleichen der Funktion
\passthrough{\lstinline!alphabeta!}.

\begin{lstlisting}[language=Python]
debug_pc_count = 0

def probcut(state, depth, heuristic, alpha, beta):
    global debug_pc_count
    if state.game_over:
        return get_utility(state)
    if depth == 0:
        debug_pc_count += 1
        return heuristic(state)

    if depth == PROBCUT_DEEP_DEPTH:
        num_p = state.num_pieces
        if num_p <= 58:
            sigma = 0.00261067 + 0.00119532 * num_p + 2.65512979e-05 * num_p**2
            bound = PERCENTILE * sigma + beta
            if probcut(state, PROBCUT_SHALLOW_DEPTH,
                       heuristic, bound-1, bound) >= bound:
                return beta
            
            bound = -PERCENTILE * sigma + alpha
            if probcut(state, PROBCUT_SHALLOW_DEPTH,
                       heuristic, bound, bound+1) <= bound:
                return alpha

    moves = state.possible_moves
    child_states = [make_move(state, move) for move in moves]
    ordered_moves = []
    for child_state in child_states:
        cached = transposition_table.get(
            (child_state.board.tobytes(), child_state.turn, heuristic),
            (heuristic(child_state), 0)
        )
        ordered_moves.append((cached[0], child_state, cached[1]))
    ordered_moves.sort(reverse=(state.turn == WHITE))

    if state.turn == WHITE:
        # maximizing
        utility = -math.inf
    else:
        # minimizing
        utility = math.inf

    for (_, tmp_state, cached_depth) in ordered_moves:
        tmp_utility = probcut(tmp_state, depth - 1, heuristic, alpha, beta)
        if depth - 1 > cached_depth:
            transposition_table[
                (tmp_state.board.tobytes(),
                 tmp_state.turn, heuristic)
            ] = (tmp_utility, depth -1)

        if state.turn == WHITE:
            # maximizing
            utility = max(utility, tmp_utility)
            alpha = max(alpha, utility)
        else:
            # minimizing
            utility = min(utility, tmp_utility)
            beta = min(beta, utility)
        if alpha >= beta:
            break  # alpha-beta pruning
    return utility
\end{lstlisting}

\hypertarget{durchfuxfchren-der-zuxfcge}{%
\subsection{Durchführen der Züge}\label{durchfuxfchren-der-zuxfcge}}

Die folgenden Funktionen berechnen mithilfe einer angegebenen KI
Strategie den nächsten Zug und wenden diesen auf den übergebenen Zustand
\passthrough{\lstinline!state!} an. Damit die Strategien nicht völlig
deterministisch sind, und somit besser die Stärke der einzelnen
Strategien und Heuristiken bestimmt werden kann, wird nicht immer der
beste Zug ausgewählt, sondern stattdessen einer der Züge, die innerhalb
eines festgelegten Abstands vom besten Zug liegen. Dieser Abstand wird
im Folgenden als \passthrough{\lstinline!SELECTION\_TOLERANCE!}
definiert.

\begin{lstlisting}[language=Python]
SELECTION_TOLERANCE = 0.0001
\end{lstlisting}

\hypertarget{suche-mit-fester-tiefe}{%
\subsubsection{Suche mit fester Tiefe}\label{suche-mit-fester-tiefe}}

Die Funktion \passthrough{\lstinline!ai\_make\_move!} ist die einfachste
der Ausführungsfunktionen. Sie bewertet alle, durch einen Zug vom
Zustand \passthrough{\lstinline!state!} erreichbaren, Spielpositionen
und wählt aus diesen, wie oben beschrieben, einen der besten Züge aus.
Die Bewertung der Spielzustände wird von der als Parameter übergebenen
Funktion \passthrough{\lstinline!ai!} vorgenommen, welche eine der im
vorherigen Abschnitt definierten Strategie-Funktionen sein kann. Für
jeden Zustand wird die Strategie-Funktion genau einmal mit der Tiefe
\passthrough{\lstinline!depth-1!} ausgeführt. Das
\passthrough{\lstinline!-1!} wird hierbei verwendet, da bereits in der
Funktion \passthrough{\lstinline!ai\_make\_move!} selbst eine Iteration
über die Kindzustände durchgeführt wird. Die Strategiefunktion erhält
ausßerdem den übergebenen Parameter \passthrough{\lstinline!heuristic!},
welche eine der implementierten Heuristikfunktionen sein kann.

\begin{lstlisting}[language=Python]
def ai_make_move(ai, state, depth, heuristic):
    global utilities
    if state.game_over:
        return
    scored_moves = []
    if state.turn == WHITE:
        # maximizing
        for move in state.possible_moves:
            new_state = make_move(state, move)
            utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)
            scored_moves.append((utility, move))
        best_score, _ = max(scored_moves)
    else:
        # minimizing
        for move in state.possible_moves:
            new_state = make_move(state, move)
            utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)
            scored_moves.append((utility, move))
        best_score, _ = min(scored_moves)
    utilities[state.turn] = best_score
    top_moves = [move for move in scored_moves
                 if abs(move[0] - best_score) <= SELECTION_TOLERANCE]
    best_move = random.choice(top_moves)[1]
    return make_move(state, best_move)
\end{lstlisting}

\hypertarget{iterative-tiefensuche}{%
\subsubsection{Iterative Tiefensuche}\label{iterative-tiefensuche}}

Die Ausführungsfunktion \passthrough{\lstinline!ai\_make\_move\_id!}
unterscheidet sich von \passthrough{\lstinline!ai\_make\_move!} dadurch,
dass iterative Tiefensuche durchgeführt wird. Dazu wird die
Strategiefunktion, statt nur einmal mit der vorgegebenen Tiefe
aufgerufen zu werden, beginnend von 1 mit immer höherer Suchtiefe
aufgerufen. Wird die Tiefe \passthrough{\lstinline!depth!} erreicht, so
wird einer der besten Züge, wie auch in
\passthrough{\lstinline!ai\_make\_move!} ausgewählt. Durch die
Verwendung eines Cache, der
\passthrough{\lstinline!transposition\_table!}, in den auf Alpha-Beta
Prunning basierenden Strategien, kann durch die Wiederverwendung der
Ergebnisse vorheriger Aufrufe ein besseres Move Ordering vorgenommen
werden, und somit die Effizienz des Alpha-Beta Pruning gesteigert
werden. Bei ausreichender Suchtiefe übertreffen die dadurch erzielten
Ersparnisse, den zusätzlichen Aufwand, die Strategiefunktionen mehrfach
aufzurufen.

\begin{lstlisting}[language=Python]
def ai_make_move_id(ai, state, depth, heuristic):
    global utilities
    if state.game_over:
        return
    best_move = None
    cur_depth = 1
    while cur_depth <= depth:
        scored_moves = []
        if state.turn == WHITE:
            # maximizing
            for move in state.possible_moves:
                new_state = make_move(state, move)
                utility = ai(new_state, cur_depth-1, heuristic, -math.inf, math.inf)
                scored_moves.append((utility, move))
            best_score, _ = max(scored_moves)
        else:
            # minimizing
            for move in state.possible_moves:
                new_state = make_move(state, move)
                utility = ai(new_state, cur_depth-1, heuristic, -math.inf, math.inf)
                scored_moves.append((utility, move))
            best_score, _ = min(scored_moves)
        utilities[state.turn] = best_score
        top_moves = [move for move in scored_moves
                     if abs(move[0] - best_score) <= SELECTION_TOLERANCE]
        best_move = random.choice(top_moves)[1]
        cur_depth += 1
    return make_move(state, best_move)
\end{lstlisting}

\hypertarget{zeitbeschruxe4nkte-tiefensuche}{%
\subsubsection{Zeitbeschränkte
Tiefensuche}\label{zeitbeschruxe4nkte-tiefensuche}}

Je nach Spielsituation ist die Mobilität der Spieler unterschiedlich
hoch. Dadurch unterscheidet sich auch die Anzahl der zu betrachteten
Spielzustände. Auch die Anzahl der durch Alpha-Beta Pruning entfernten
Zweige kann variieren. Bei konstanter Suchtiefe ist daher mit variablen
Ausführungszeiten zu rechnen. Im Spiel gegen einen Menschlichen Spieler
ist es jedoch wünschenswert, eine maximale Zugdauer nicht zu
überschreiten. Die verfügbare Zeit soll dabei dennoch effektiv für eine
möglichst gute Entscheidung genutzt werden.

Das ist das Ziel der Ausführungsfunktion
\passthrough{\lstinline!ai\_make\_move\_id\_timelimited!}, diese führt
Iterative Deepening durch, kann jedoch nach jeder Suchtiefe abbrechen
und einen der bis dahin besten Züge wählen. Hierbei wird die
Entscheidung zum Abbruch getroffen, wenn mit der nächsten Ausführung das
durch den Parameter \passthrough{\lstinline!timelimit!} gegebene
Zeitlimit voraussichtlich überschritten würde. Dafür wird die Dauer der
nächsten Ausführung approximiert, indem bestimmt wird, um welchen Faktor
sich die Ausführungszeiten bei den letzten beiden Ausführungen geändert
haben. Dieser Faktor \passthrough{\lstinline!factor!} wird dann mit der
Dauer der letzten Ausführung multipliziert um die Dauer der nächsten
Ausführung zu schätzen. Zu beachten ist, dass diese Funktion nicht exakt
die gleiche Schnittstelle hat, wie die anderen Ausführungsfunktionen.
Der paramenter \passthrough{\lstinline!depth!} wurde hier durch das
\passthrough{\lstinline!timelimit!} ersetzt. Dies ist beim Aufruf der
Funktion zu beachten.

\begin{lstlisting}[language=Python]
def ai_make_move_id_timelimited(ai, state, timelimit, heuristic):
    global utilities
    if state.game_over:
        return
    best_move = None
    depth = 1
    last_time = 1
    second_last_time = 1
    factor = 0
    start = time.time()
    while (
        depth <= 64 - state.num_pieces and
        timelimit - (time.time() - start) >= factor * last_time
    ):
        last_time_start = time.time()
        scored_moves = []
        if state.turn == WHITE:
            # maximizing
            for move in state.possible_moves:
                new_state = make_move(state, move)
                utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)
                scored_moves.append((utility, move))
            best_score, _ = max(scored_moves)
        else:
            # minimizing
            for move in state.possible_moves:
                new_state = make_move(state, move)
                utility = ai(new_state, depth-1, heuristic, -math.inf, math.inf)
                scored_moves.append((utility, move))
            best_score, _ = min(scored_moves)
        utilities[state.turn] = best_score
        top_moves = [move for move in scored_moves
                     if abs(move[0] - best_score) <= timelimit]
        best_move = random.choice(top_moves)[1]
        second_last_time = last_time
        last_time = time.time() - last_time_start
        factor = min(last_time / second_last_time, 3)
        depth += 1
    print("Reached depth", depth-1, "in", time.time() - start, "seconds")
    return make_move(state, best_move)
\end{lstlisting}
